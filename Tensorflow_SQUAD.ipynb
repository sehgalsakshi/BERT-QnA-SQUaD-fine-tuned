{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow SQUAD.ipynb",
      "provenance": [],
      "mount_file_id": "1o_OKN3h_DPiDQzGn4966SGZJjKUvdIWO",
      "authorship_tag": "ABX9TyPrdXbzswu0iIvEYCrAWyMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sehgalsakshi/BERT-QnA-SQUaD-fine-tuned/blob/main/Tensorflow_SQUAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iG8ETPDWn_n"
      },
      "source": [
        "#Fine Tuning BERT for question answering task (given a context to find answers for)\r\n",
        "### Pretrained Model is being taken from tensorflow hub. Further fine tuning is done as per our parameters\r\n",
        "\r\n",
        "Input sequence is special start token + context string + separator token + question + sepator token\r\n",
        "\r\n",
        "Output is start and end logits (indexes with highest probability of being the answer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz0bqdWXKoqj",
        "outputId": "8843cb42-64ae-4e93-8c5a-83569cc15a66"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.9.4)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->-r requirements.txt (line 1)) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Hj4qdCiAFa"
      },
      "source": [
        "#importing the required libraries\r\n",
        "import json\r\n",
        "import os\r\n",
        "import re\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZKnEj3jSXdO"
      },
      "source": [
        "======================================== PREPARING DATASET ==================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-_EMoAziMHI"
      },
      "source": [
        "class Sample:\r\n",
        "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\r\n",
        "        self.question = question\r\n",
        "        self.context = context\r\n",
        "        self.start_char_idx = start_char_idx\r\n",
        "        self.answer_text = answer_text\r\n",
        "        self.all_answers = all_answers\r\n",
        "        self.skip = False\r\n",
        "        self.start_token_idx = -1\r\n",
        "        self.end_token_idx = -1\r\n",
        "\r\n",
        "    def preprocess(self):\r\n",
        "        #  clean context and question\r\n",
        "        context = \" \".join(str(self.context).split())\r\n",
        "        question = \" \".join(str(self.question).split())\r\n",
        "        # tokenize context and question -- appends cls at beginning and sep at end\r\n",
        "        tokenized_context = tokenizer.encode(context)  \r\n",
        "        tokenized_question = tokenizer.encode(question)\r\n",
        "        # if this is validation or training sample, preprocess answer\r\n",
        "        if self.answer_text is not None:\r\n",
        "            answer = \" \".join(str(self.answer_text).split())\r\n",
        "            end_char_idx = self.start_char_idx + len(answer)\r\n",
        "            # check if end character index is in the context\r\n",
        "            if end_char_idx >= len(context):\r\n",
        "                self.skip = True\r\n",
        "                return\r\n",
        "            # mark all the character indexes in context that are also in answer \r\n",
        "            is_char_in_ans = [0] * len(context)\r\n",
        "            for idx in range(self.start_char_idx, end_char_idx):\r\n",
        "                is_char_in_ans[idx] = 1\r\n",
        "            ans_token_idx = []\r\n",
        "            for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n",
        "                if sum(is_char_in_ans[start:end]) > 0:\r\n",
        "                    # find all the tokens that are in the answers\r\n",
        "                    ans_token_idx.append(idx)\r\n",
        "            if len(ans_token_idx) == 0:\r\n",
        "                self.skip = True\r\n",
        "                return\r\n",
        "            # get start and end token indexes\r\n",
        "            self.start_token_idx = ans_token_idx[0]\r\n",
        "            self.end_token_idx = ans_token_idx[-1]\r\n",
        "        # create inputs \r\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:] \r\n",
        "        #here 0th index is removed because we do not want to add cls token again\r\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\r\n",
        "        attention_mask = [1] * len(input_ids)\r\n",
        "        padding_length = max_seq_length - len(input_ids)\r\n",
        "        # add padding if necessary\r\n",
        "        if padding_length > 0:\r\n",
        "            input_ids = input_ids + ([0] * padding_length)\r\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "        elif padding_length < 0:\r\n",
        "            self.skip = True\r\n",
        "            return\r\n",
        "        self.input_word_ids = input_ids\r\n",
        "        self.input_type_ids = token_type_ids\r\n",
        "        self.input_mask = attention_mask\r\n",
        "        self.context_token_to_char = tokenized_context.offsets"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_A_TghYSt8y"
      },
      "source": [
        "============================================= CREATING INPUT SEQUENCE =========================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16j_7lcDiYLH"
      },
      "source": [
        "def create_squad_examples(raw_data):\r\n",
        "    squad_examples = []\r\n",
        "    for item in raw_data[\"data\"]:\r\n",
        "        for para in item[\"paragraphs\"]:\r\n",
        "            context = para[\"context\"]\r\n",
        "            for qa in para[\"qas\"]:\r\n",
        "                question = qa[\"question\"]\r\n",
        "                if \"answers\" in qa:\r\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\r\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\r\n",
        "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\r\n",
        "                    squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\r\n",
        "                else:\r\n",
        "                    squad_eg = Sample(question, context)\r\n",
        "                squad_eg.preprocess()\r\n",
        "                squad_examples.append(squad_eg)\r\n",
        "    return squad_examples\r\n",
        "\r\n",
        "\r\n",
        "def create_inputs_targets(squad_examples):\r\n",
        "    dataset_dict = {\r\n",
        "        \"input_word_ids\": [],\r\n",
        "        \"input_type_ids\": [],\r\n",
        "        \"input_mask\": [],\r\n",
        "        \"start_token_idx\": [],\r\n",
        "        \"end_token_idx\": [],\r\n",
        "    }\r\n",
        "    for item in squad_examples:\r\n",
        "        if item.skip == False:\r\n",
        "            for key in dataset_dict:\r\n",
        "                dataset_dict[key].append(getattr(item, key))\r\n",
        "    for key in dataset_dict:\r\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\r\n",
        "    x = [dataset_dict[\"input_word_ids\"],\r\n",
        "         dataset_dict[\"input_mask\"],\r\n",
        "         dataset_dict[\"input_type_ids\"]]\r\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\r\n",
        "    return x, y"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnOSmlStS44z"
      },
      "source": [
        "============================================= TRAINING ======================================================"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO3S1Ck7ih82"
      },
      "source": [
        "class ValidationCallback(keras.callbacks.Callback):\r\n",
        "\r\n",
        "    def normalize_text(self, text):\r\n",
        "        #lowercase text\r\n",
        "        text = text.lower()\r\n",
        "        # remove redundant whitespaces\r\n",
        "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\r\n",
        "        #remove articles\r\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\r\n",
        "        text = re.sub(regex, \" \", text)\r\n",
        "        text = \" \".join(text.split())\r\n",
        "        return text\r\n",
        "\r\n",
        "    def __init__(self, x_eval, y_eval):\r\n",
        "        self.x_eval = x_eval\r\n",
        "        self.y_eval = y_eval\r\n",
        "\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        # get the offsets of the first and last tokens of predicted answers\r\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\r\n",
        "        count = 0\r\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\r\n",
        "        # for every pair of offsets\r\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "            # take the required Sample object with the ground-truth answers in it\r\n",
        "            squad_eg = eval_examples_no_skip[idx]\r\n",
        "            # use offsets to get back the span of text corresponding to\r\n",
        "            # our predicted first and last tokens\r\n",
        "            offsets = squad_eg.context_token_to_char\r\n",
        "            start = np.argmax(start)\r\n",
        "            end = np.argmax(end)\r\n",
        "            if start >= len(offsets):\r\n",
        "                continue\r\n",
        "            pred_char_start = offsets[start][0]\r\n",
        "            if end < len(offsets):\r\n",
        "                pred_char_end = offsets[end][1]\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\r\n",
        "            else:\r\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\r\n",
        "            normalized_pred_ans = self.normalize_text(pred_ans)\r\n",
        "            # clean the real answers\r\n",
        "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\r\n",
        "            # check if the predicted answer is in an array of the ground-truth answers\r\n",
        "            if normalized_pred_ans in normalized_true_ans:\r\n",
        "                count += 1\r\n",
        "        acc = count / len(self.y_eval[0])\r\n",
        "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sBsizcQTLYX"
      },
      "source": [
        "============================================= FETCHING DATASET ================================================"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yRv3d77kN8C"
      },
      "source": [
        "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\r\n",
        "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\r\n",
        "with open(train_path) as f: raw_train_data = json.load(f)\r\n",
        "with open(eval_path) as f: raw_eval_data = json.load(f)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyO0tmI_kW6J"
      },
      "source": [
        "#max length of input. If it crossess this, doc stride (sliding window concept is used)\r\n",
        "max_seq_length = 384\r\n",
        "\r\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\r\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\r\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\r\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZbFQarJUnKy"
      },
      "source": [
        "============================================ SETTING TRAINING PARAMS ============================================"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRzVhKvWiovU",
        "outputId": "dbaf974b-6cbb-4882-b5a5-735425983f98"
      },
      "source": [
        "train_squad_examples = create_squad_examples(raw_train_data)\r\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\r\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\r\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\r\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\r\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")\r\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\r\n",
        "start_logits = layers.Flatten()(start_logits)\r\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\r\n",
        "end_logits = layers.Flatten()(end_logits)\r\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\r\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\r\n",
        "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\r\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n",
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\r\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss])\r\n",
        "model.summary()\r\n",
        "model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_eval, y_eval)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87599 training points created.\n",
            "10570 evaluation points created.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       768         keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       768         keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,777\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "10669/10669 [==============================] - ETA: 0s - loss: 5.4535 - activation_loss: 2.8604 - activation_1_loss: 2.5932WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fba13ac2ea0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fba13ac2ea0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fba13ac2ea0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\n",
            "epoch=1, exact match score=0.41\n",
            "10669/10669 [==============================] - 9068s 850ms/step - loss: 5.4535 - activation_loss: 2.8604 - activation_1_loss: 2.5932\n",
            "Epoch 2/2\n",
            " 1495/10669 [===>..........................] - ETA: 2:05:06 - loss: 4.1548 - activation_loss: 2.1919 - activation_1_loss: 1.9629"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7qTC2AtViOv"
      },
      "source": [
        "=========================================== SAVING MODEL AND IT'S STATE ============================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeoQQSGsVrVh"
      },
      "source": [
        "# saving model and architecture to single file\r\n",
        "model.save(\"model.h5\")\r\n",
        "#to be used later... \r\n",
        "#from tensorflow.keras.models import load_model\r\n",
        "# load model\r\n",
        "#model = load_model('model.h5', custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "\r\n",
        "#saving model weights\r\n",
        "model.save_weights(\"./weights.h5\")\r\n",
        "# serialize model to JSON\r\n",
        "model_json = model.to_json()\r\n",
        "with open(\"model.json\", \"w\") as json_file:\r\n",
        "    json_file.write(model_json)\r\n",
        "\r\n",
        "# later...\r\n",
        "#from tensorflow.keras.models import model_from_json\r\n",
        " \r\n",
        "# load json and create model\r\n",
        "#json_file = open('model.json', 'r')\r\n",
        "#loaded_model_json = json_file.read()\r\n",
        "#json_file.close()\r\n",
        "#loaded_model = model_from_json(loaded_model_json, custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# load weights into new model\r\n",
        "#loaded_model.load_weights(\"weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLv7HqNFU28U"
      },
      "source": [
        "==================================================== TESTING ====================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLXW-gcgop8w"
      },
      "source": [
        "data = {\"data\":\r\n",
        "    [\r\n",
        "        {\"title\": \"Project Apollo\",\r\n",
        "         \"paragraphs\": [\r\n",
        "             {\r\n",
        "                 \"context\": \"The Apollo program, also known as Project Apollo, was the third United States human \"\r\n",
        "                            \"spaceflight program carried out by the National Aeronautics and Space Administration (\"\r\n",
        "                            \"NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First \"\r\n",
        "                            \"conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to \"\r\n",
        "                            \"follow the one-man Project Mercury which put the first Americans in space, Apollo was \"\r\n",
        "                            \"later dedicated to President John F. Kennedy's national goal of landing a man on the \"\r\n",
        "                            \"Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in \"\r\n",
        "                            \"a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project \"\r\n",
        "                            \"Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, \"\r\n",
        "                            \"and was supported by the two man Gemini program which ran concurrently with it from 1962 \"\r\n",
        "                            \"to 1966. Gemini missions developed some of the space travel techniques that were \"\r\n",
        "                            \"necessary for the success of the Apollo missions. Apollo used Saturn family rockets as \"\r\n",
        "                            \"launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications \"\r\n",
        "                            \"Program, which consisted of Skylab, a space station that supported three manned missions \"\r\n",
        "                            \"in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the \"\r\n",
        "                            \"Soviet Union in 1975.\",\r\n",
        "                 \"qas\": [\r\n",
        "                     {\"question\": \"What project put the first Americans into space?\",\r\n",
        "                      \"id\": \"Q1\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What program was created to carry out these projects and missions?\",\r\n",
        "                      \"id\": \"Q2\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What year did the first manned Apollo flight occur?\",\r\n",
        "                      \"id\": \"Q3\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\",\r\n",
        "                      \"id\": \"Q4\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"How long did Project Apollo run?\",\r\n",
        "                      \"id\": \"Q5\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What program helped develop space travel techniques that Project Apollo used?\",\r\n",
        "                      \"id\": \"Q6\"\r\n",
        "                      },\r\n",
        "                     {\"question\": \"What space station supported three manned missions in 1973-1974?\",\r\n",
        "                      \"id\": \"Q7\"\r\n",
        "                      }\r\n",
        "                 ]}]}]}\r\n",
        "\r\n",
        "test_samples = create_squad_examples(data)\r\n",
        "x_test, _ = create_inputs_targets(test_samples)\r\n",
        "pred_start, pred_end = model.predict(x_test)\r\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\r\n",
        "    test_sample = test_samples[idx]\r\n",
        "    offsets = test_sample.context_token_to_char\r\n",
        "    start = np.argmax(start)\r\n",
        "    end = np.argmax(end)\r\n",
        "    pred_ans = None\r\n",
        "    if start >= len(offsets):\r\n",
        "        continue\r\n",
        "    pred_char_start = offsets[start][0]\r\n",
        "    if end < len(offsets):\r\n",
        "        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\r\n",
        "    else:\r\n",
        "        pred_ans = test_sample.context[pred_char_start:]\r\n",
        "    print(\"Q: \" + test_sample.question)\r\n",
        "    print(\"A: \" + pred_ans if pred_ans is not None else 'Cannot be determined from given context')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}